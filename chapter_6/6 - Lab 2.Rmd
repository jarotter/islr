---
title: "ISLR 6 - Lab 2: Ridge Regresion and the Lasso"
output: html_notebook
---

This lab on Ridge Regression and the Lasso in R comes from p. 251-255 of "Introduction to Statistical Learning with Applications in R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was adapted for use of the `caret` package.

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(ISLR)
library(plyr)
library(caret)
library(dplyr)
```


# 6.6: Ridge Regression and the Lasso

We will use the `glmnet` package in order to perform ridge regression and the lasso. The main function in 
this package is `glmnet()`, which can be used to fit ridge regression models, lasso models, and more. This 
function has slightly different syntax from other model-fitting functions that we have encountered thus far in 
this book. In particular, we must pass in an $x$ matrix as well as a $y$ vector, and we do not use the formula 
syntax.

Before proceeding, let's first ensure that the missing values have been removed from the data, as described in 
the previous lab.

```{r}
Hitters <- na.omit(Hitters)
```

We will now perform ridge regression and the lasso in order to predict `Salary` on the `Hitters` data. Let's set 
up our data:

```{r}
x <- model.matrix(Salary~., data = Hitters)[,-1]
y <- Hitters$Salary
```

The `model.matrix()` function is particularly useful for creating $x$; not only does it produce a matrix 
corresponding to the 19 predictors but it also automatically transforms any qualitative variables into dummy 
variables. The latter property is important because `glmnet()` can only take numerical, quantitative inputs.

# 6.6.1 Ridge Regression
The `glmnet()` function has an alpha argument that determines what type of model is fit. If ${\tt alpha=0}$ then 
a ridge regression model is fit, and if ${\tt alpha=1}$ then a lasso model is fit. We first fit a ridge 
regression model:

```{r}
tune_grid <- expand.grid(.alpha = 0,
                         .lambda = 10^seq(10,-2,length=100))

train_control <- trainControl(method = "repeatedcv",
                              repeats = 5,
                              number = 10)
```

By default the `glmnet()` function performs ridge regression for an automatically selected range of $\lambda$ 
values. However, here we have chosen to implementthe function over a grid of values ranging from 
$\lambda = 10^10$ to $\lambda = 10^{-2}$, essentially covering the full range of scenarios from the null model 
containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a 
particular value of $\lambda$ that is not one of the original grid values. Note that by default, the `glmnet()`
function standardizes the variables so that they are on the same scale. To turn off this default setting,
use the argument ${\tt standardize=FALSE}$.

We now split the samples into a training set and a test set in order to estimate the test error of ridge 
regression and the lasso. There are two common ways to randomly split a data set:

 - The first is to produce a random vector of ${\tt TRUE, FALSE}$ elements and select the observations 
 corresponding to ${\tt TRUE}$ for the training data. 
 
 - The second is to randomly choose a subset of numbers between $1$ and $n$; these can then be used as the indices for the training observations. 
 
The two approaches work equally well. We used the former method in the previous lab. Here we demonstrate the 
latter approach. We first set a random seed so that the results obtained will be reproducible.

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y_test <- y[test]
```

And now we fit a model using only this train data:

```{r}
set.seed(1)
trad_model_nl <- cv.glmnet(x = x[train,], y = y[train],alpha = 0)
ridge_model_train <- train(x = x[train, ],
                     y = y[train],
                     method = 'glmnet',
                     tuneGrid = tune_grid,
                     trControl = train_control)
```

We saw that the value of $\lambda$ that results in the smallest cross-validation error is 403 What is the test 
MSE associated with this value of $\lambda$?

```{r}
set.seed(1)
ridge_train_predictions  <- predict(ridge_model_train$finalModel,
                              s = ridge_model_train$bestTune$lambda,
                              newx = x[test,])
mean((ridge_train_predictions-y_test)^2) 

ridge_trad_predictions <- predict(trad_model_nl,
                                  s = trad_model_nl$lambda.min,
                                  newx = x[test,])

mean((ridge_trad_predictions - y_test)^2)
```

To obtain the actual coefficients of the model, we can use the `coef` function as follows:

```{r}
coef(ridge_model$finalModel, ridge_model_train$bestTune$lambda)
```
Note that the second argument specifies the value of $\lambda$ we want to use.
As expected, none of the coefficients are exactly zero - ridge regression does not
perform variable selection!

# 6.6.2 The Lasso
We saw that ridge regression with a wise choice of $\lambda$ can outperform least squares as well as the null 
model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more 
interpretable model than ridge regression. In order to fit a lasso model, we once again use the `glmnet()` 
function; however, this time we use the argument ${\tt alpha=1}$. Other than that change, we proceed just as we 
did in fitting a ridge model:

```{r}
set.seed(1)
lasso_grid <- expand.grid(.alpha = 1, .lambda = 10^seq(10,-2,length=100))

lasso_model <- train(x = x[train,],
                     y = y[train],
                     tuneGrid = lasso_grid,
                     trControl = train_control,
                     method = 'glmnet')
plot(lasso_model)
plot(lasso_model$finalModel)
```

Notice that in the coefficient plot that depending on the choice of tuning parameter, some of the coefficients 
are exactly equal to zero. We now perform cross-validation and compute the associated test error in the 
old-fashioned way:

```{r}
set.seed(1)
cv_out <- cv.glmnet(x[train,],
                    y[train],
                    alpha=1)          # Fit lasso model on training data
plot(cv_out)                                          # Draw plot of training MSE as a function of lambda
bestlam <- cv_out$lambda.min                             # Select lamda that minimizes training MSE
lasso_pred <- predict(lasso_model$finalModel,
                      s = bestlam,
                      newx = x[test,]) # Use best lambda to predict test data
mean((lasso_pred-y_test)^2)                           # Calculate test MSE
```

This is substantially lower than the test set MSE of the null model and of least squares, and very similar to the 
test MSE of ridge regression with $\lambda$ chosen by cross-validation.

However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates 
are sparse. Here we see that 12 of the 19 coefficient estimates are exactly zero:

```{r}
out <- train(x,y,
              trControl = train_control,
              method = 'glmnet',
              tuneGrid = lasso_grid)                          # Fit lasso model on full dataset
lasso_coefficients <- coef(out$finalModel, bestlam) # Display coefficients using lambda chosen by CV
lasso_coefficients
```

Selecting only the predictors with non-zero coefficients, we see that the lasso model with $\lambda$
chosen by cross-validation contains only seven variables:

```{r}
lasso_coefficients[lasso_coefficients!=0]                                    # Display only non-zero coefficients
```