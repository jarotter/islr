---
title: "ISLR 6 - Lab 2: PCR and PLS Regresion"
output: html_notebook
---

This lab on PCS and PLS in R comes from p. 256-259 of "Introduction to Statistical Learning with Applications in 
R" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. It was adapted for use of the `caret` 
library where suited.

# 6.7.1 Principal Components Regression

Principal components regression (PCR) can be performed using the `pcr()`
function, which is part of the ${\tt pls}$ library. In this lab, we'll apply PCR to the `Hitters`
data, in order to predict ${\tt Salary}$. As in previous labs, we'll start by ensuring that the missing values 
havebeen removed from the data:

```{r}
library(ISLR)
library(pls)
Hitters <- na.omit(Hitters) # Omit empty rows
```

The syntax for the `pcr()` function is similar to that for $`lm()` with a few additional options. Setting 
${\tt scale=TRUE}$ has the effect of standardizing each predictor prior to generating the principal components, 
so that the scale on which each variable is measured will not have an effect. Setting ${\tt validation="CV"}$ 
causes `pcr()`to compute the ten-fold cross-validation errorfor each possible value of $M$, the number of   
components used.  As usual, we'll set a random seed for consistency:

In `caret`, the `preProcess` argument to `train()` can specify PCA analysis by adding `"pca"`to the vector 
passed to it. As we see below, using `lm` or `glm` models yield the same results. However, the `pcr` package is
compatible with `caret` too.

```{r}
set.seed(2)
pcr_classic <- pcr(Salary~., data = Hitters, scale = TRUE, validation = "CV")
train_control <- trainControl(method = "repeatedcv",
                              number = 10,
                              repeats = 5)
pcr_caret_lm <- train(Salary ~ .,
                   data = Hitters,
                   preProcess = c("center", "scale", "pca"),
                   method = 'lm',
                   trControl = train_control)

pcr_caret_glm <- train(Salary ~ .,
                   data = Hitters,
                   preProcess = c("center", "scale", "pca"),
                   method = 'glm',
                   trControl = train_control)

pcr_caret <- train(Salary ~ .,
                   data = Hitters,
                   scale = TRUE,
                   validation = 'CV',
                   method = 'pcr',
                   tuneGrid = expand.grid(.ncomp = 1:(ncol(Hitters)-1)),
                   trControl = train_control)


```

The resulting fit can be examined using the ${\tt summary()}$ function:

```{r}
summary(pcr_classic)
summary(pcr_caret$finalModel)
```

The CV score is provided for each possible number of components, ranging from $M = 0$ onwards. Note that `pcr()` 
reports the **root mean squared error**; in order to obtain the usual MSE, we must square this quantity. For 
instance, a root mean squared error of 352.8 corresponds to an MSE of 352.82 = 124,468.

One can also plot the cross-validation scores using the `validationplot()` function. Using 
${\tt val.type="MSEP"}$ will cause the cross-validation MSE to be plotted:

```{r}
validationplot(pcr_classic)
validationplot(pcr_caret$finalModel)
plot(pcr_caret)
```

We see that the smallest cross-validation error occurs when $M = 16$ components are used. This is barely fewer 
than $M = 19$, which amounts to simply performing least squares, because when all of the components are used in 
PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly 
the same when only one component is included in the model. This suggests that a model that uses just a small 
number of components might suffice.

You might have noticed that the `summary()` function also provides the percentage of variance explained in the 
predictors and in the response using different numbers of components. We'll dig deeper into this concept in 
Chapter 10, but for now we can think of this as the amount of information about the predictors or the response
that is captured using $M$ principal components. For example, setting $M = 1$ only captures 38.31% of all the 
variance, or information, in the predictors. In contrast, using $M = 6$ increases the value to 88.63%. If we were 
to use all $M = p = 19$ components, this would increase to 100%.

Now let's perform PCR on the training data and evaluate its test set
performance:

```{r}
set.seed(1)
train_index <- createDataPartition(Hitters$Salary, p = 0.5, list = FALSE)
train_data <- Hitters[train_index,]

pcr_classic <- pcr(Salary~., data = Hitters, subset = train_index, scale = TRUE, validation = "CV")
validationplot(pcr_classic, val.type = "MSEP")

pcr_caret <- train(Salary ~ .,
                   data = train_data,
                   scale = TRUE,
                   validation = "CV",
                   trControl = train_control,
                   method = "pcr",
                   verbose = TRUE,
                   tuneGrid = expand.grid(.ncomp = 1:19))
validationplot(pcr_caret$finalModel, val.type = "MSEP")
ggplot(pcr_caret)
```

We find that the lowest cross-validation error occurs when $M = 7$ components are used. We compute the test MSE 
as follows:

```{r}
x <- model.matrix(Salary~.,Hitters )[,-1]
y <- Hitters$Salary

classic_pred <- predict(pcr_classic, x[-train_index,], ncomp = 7) 
sqrt(mean((classic_pred-y[-train_index])^2))

caret_pred <- predict(pcr_caret$finalModel, x[-train_index,], ncomp = 5)
sqrt(mean((caret_pred - y[-train_index])^2))

caret_pred_preProc = predict(pcr_caret_glm, Hitters[-train_index,])
sqrt(mean((caret_pred_preProc-y[-train_index])^2))
```

This test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a 
result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform
any kind of variable selection or even directly produce coefficient estimates.

Finally, we fit PCR on the full data set using $M = 7$, the number of components identified by cross-validation:
We see that when manualy setting the number of components to seven, both models are the same; but if `ncomp` isn't
specified, `caret` will chose 5.

```{r}
classic_final <- pcr(y~x, scale = TRUE, ncomp = 7)
summary(classic_final)

caret_final <- train(x = x, y = y,
                     scale = TRUE,
                     tuneGrid = expand.grid(.ncomp = 1:19),
                     trControl = train_control,
                     selectonFuntion = oneSE,
                     method = 'pcr')
summary(caret_final$finalModel)

caret_pred <- predict(caret_final$finalModel, x[-train_index,])
sqrt(mean((caret_pred - y[-train_index])^2))
```

# 6.7.2 Partial Least Squares

Next we'll implement partial least squares (PLS) using the `pls()` function, also in the `pls` library. The 
syntax is just like that of the `pcr()` function:

```{r}
set.seed(1)
pls_classic <- plsr(Salary~., data = Hitters, subset = train_index, scale = TRUE, validation = "CV")
summary(pls_classic)
validationplot(pls_classic)

pls_caret <- train(Salary ~ .,
                   data = Hitters[train_index,],
                   tuneLength = 19,
                   scale = TRUE,
                   trControl = train_control,
                   method = "pls")
summary(pls_caret$finalModel)
plot(pls_caret)
validationplot(pls_caret$finalModel)
```

The lowest cross-validation error occurs when only $M = 2$ partial least squares directions are used. We now 
evaluate the corresponding test set MSE:

```{r}
classic_pred = predict(pls_classic, x[-train_index,], ncomp = 2)
mean((classic_pred-y[-train_index])^2)

caret_pred = predict(pls_caret$finalModel, x[-train_index,], ncomp = 1)
mean((caret_pred-y[-train_index])^2)
```

The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, 
and PCR. However, using the `preProcess` argument `caret` beats both methods. Ha. 